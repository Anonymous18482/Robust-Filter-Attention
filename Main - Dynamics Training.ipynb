{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55sKZVe2NcuJ"
   },
   "source": [
    "## Robust Filter Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on synthetic data from simulated dynamical systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1IgNYUyROjm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import patches\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # Loading bar\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import complex_matmul, apply_interleaved_rope\n",
    "from utils import count_parameters, get_layers, seed_everything\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamics import simulate_stochastic_LTI, simulate_diagonalized_stochastic_LTI, DynamicSim\n",
    "from dynamics import construct_mapping\n",
    "from dynamics import get_nth_measurement, get_random_measurements\n",
    "from dynamics import linear_spiral, linear_spiral_3D, Lorenz, rand_coupling_matrix, Van_der_Pol_osc\n",
    "from dynamics import construct_random_mapping, construct_data_dynamics, TrainDataset_dynamics, create_train_loader_dynamics\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isotropic_rfa import get_safe_exp_tot, compute_covariance_matrix, compute_covariance_matrix_LHopital\n",
    "from isotropic_rfa import compute_covariance_matrix_spectral_full, compute_covariance_matrix_residual_diffusion\n",
    "from isotropic_rfa import compute_exp_kernel_isotropic, compute_residual_norm_isotropic\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import resolve_multihead_dims, autoregressive_sample\n",
    "from model import init_complexlinear, init_complex_matrix, initialize_linear_layers\n",
    "from model import init_rope, init_decay_per_head, init_linear_bias_slopes\n",
    "from model import apply_weight_masks\n",
    "from model import ComplexLinearLayer, ComplexLinearHermitianLayer, ComplextoRealLinearLayer\n",
    "from model import ComplexRMSNorm\n",
    "from model import MultiHeadAttentionLayer, MultiheadIsotropicRFA\n",
    "from model import TransformerBlock, TransformerNetwork\n",
    "from model import SelfAttentionBlock, RFA_Block\n",
    "from model import RFATransformerBlock, RFATransformerNetwork\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_trajectory, compute_state_matrix, plot_state_matrix, visualize_results\n",
    "from visualization import visualize_results_attn, _get_visual_modules\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import single_epoch_rfa_dynamics, single_epoch_attn_dynamics\n",
    "from training import hook_fn\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeWE9GM-R_40"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('DA')\n",
    "parser.add_argument('--gpu', type=int, default=0) # (Default: 0)\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(args.device)\n",
    "\n",
    "seed_everything(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load \\& Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## DEFINE THE LINERAR SYSTEM ##\n",
    "###############################\n",
    "\n",
    "# # Normal system\n",
    "# D1 = torch.zeros(2,2,2).to(args.device) # Diagonal matrix\n",
    "# D1[0] = torch.tensor([[-0.1, 0.0], [0.0, -0.1]]).to(args.device)\n",
    "# D1[1] = torch.tensor([[-1.0, 0.0], [0.0, 1.0]]).to(args.device)\n",
    "# # S1 = torch.zeros(2,2,2).to(args.device)\n",
    "# # S1[0] = torch.tensor(([1.0,1.0],[0.0,0.0]))\n",
    "# # S1[1] = torch.tensor(([0.0,0.0],[1.0,-1.0]))\n",
    "# # S1 = U/np.sqrt(2)\n",
    "# alpha = np.random.uniform(low=0.0, high=1.0)*2*np.pi\n",
    "# beta = np.random.uniform(low=0.0, high=1.0)*2*np.pi\n",
    "# S1 = construct_special_2D_unitary(alpha=alpha, beta=beta)\n",
    "# Si1 = complex_conj_transpose(S1)\n",
    "\n",
    "# --------------------------------------\n",
    "\n",
    "# Stable 2D linear system (in diagonalized form):\n",
    "D1 = torch.zeros(2,2,2).to(args.device) # Diagonal matrix\n",
    "S1 = torch.zeros(2,2,2).to(args.device) # RHS matrix\n",
    "Si1 = torch.zeros(2,2,2).to(args.device) # Inverse of RHS matrix\n",
    "D1[0] = torch.tensor([[-0.1, 0.0], [0.0, -0.1]]).to(args.device) # Negative real part eigenvals\n",
    "# D1[0] = torch.tensor([[0.0, 0.0], [0.0, 0.0]]).to(args.device) # Zero real part eigenvals\n",
    "# D1[0] = torch.tensor([[0.1, 0.0], [0.0, 0.1]]).to(args.device) # Positive real part eigenvals\n",
    "D1[1] = torch.tensor([[-1.0, 0.0], [0.0, 1.0]]).to(args.device)\n",
    "S1[0] = torch.tensor([[1.0, 1.0], [1.0, 1.0]]).to(args.device)\n",
    "S1[1] = torch.tensor([[-1.0, 1.0], [0.0, 0.0]]).to(args.device)\n",
    "Si1[0] = 0.5*torch.tensor([[0.0, 1.0], [0.0, 1.0]]).to(args.device)\n",
    "Si1[1] = 0.5*torch.tensor([[1.0, -1.0], [-1.0, 1.0]]).to(args.device)\n",
    "A = complex_matmul(S1,complex_matmul(D1,Si1))[0].unsqueeze(0)\n",
    "params = [D1, S1, Si1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aJOYmJ79Wmi6",
    "outputId": "d306dcec-02cd-4168-b222-0f1b3bbbc1f7"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "## DYNAMICS SIMULATION OPTIONS ##\n",
    "#################################\n",
    "\n",
    "args.rand_embed = 0 # Use random orthogonal matrix when projecting data to higher dimension?\n",
    "args.weight_mask = False # Mask the weights? (0 or 1) (for visualizing in 2D)\n",
    "\n",
    "args.m = 2 # Dimension of simulated system\n",
    "args.d_e = 256 # Embedding dimension\n",
    "\n",
    "args.tf = 10.0 # Final time\n",
    "args.dt = 0.01 # Time step size\n",
    "args.n = 10 # nth measurement (ie use every nth point as a measurement)\n",
    "args.delta_t = args.dt * args.n\n",
    "\n",
    "# Process and measurement noise:\n",
    "sigma_process = 0.5 # Process noise\n",
    "sigma_measure = 1.0 # Measurement noise\n",
    "\n",
    "args.N_t = int(args.tf/args.dt) # Number of time steps\n",
    "args.k_step = 1\n",
    "args.N_t_tot = args.N_t + args.n * args.k_step\n",
    "# t_vector = (torch.arange(args.N_t_tot + 1)).to(args.device) # Array of time steps\n",
    "t_vector = (torch.arange(args.N_t_tot + 1)*args.dt).to(args.device) # Array of time steps\n",
    "args.seq_len = int(args.N_t/args.n) + 1 # Number of measurements\n",
    "args.total_L = args.seq_len + args.k_step\n",
    "\n",
    "args.batch_size = 32 # Batch size\n",
    "args.num_batch = 1 # Number of batches\n",
    "args.num_samp = args.batch_size * args.num_batch # Number of samples in train loader\n",
    "\n",
    "args.t_equal = True # Equal time intervals?\n",
    "\n",
    "# if args.t_equal == 1:\n",
    "#     args.total_time = args.seq_len - 1\n",
    "# else:\n",
    "#     args.total_time = args.tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test dynamic sim\n",
    "\n",
    "# # Initial condition\n",
    "# theta0 = torch.rand(1) * 2 * np.pi\n",
    "# r0 = 20.0 + (torch.rand(1) * 2*3.0) - 3.0\n",
    "# x0 = torch.tensor([r0 * torch.cos(theta0), r0 * torch.sin(theta0)])\n",
    "# x0 = x0.to(args.device).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "# X, X_measure = simulate_stochastic_LTI(A, x0, args.N_t_tot, args, sigma_process, sigma_measure)\n",
    "# # X, X_measure = simulate_diagonalized_stochastic_LTI(D1, S1, Si1, x0, t_vector, sigma_process, sigma_measure, args.device)\n",
    "\n",
    "# # Actual trajectory\n",
    "# X_plt = X.squeeze().detach().cpu().numpy()\n",
    "# plt.plot(X_plt.T[0],X_plt.T[1],'black')\n",
    "\n",
    "# # Noisy trajectory\n",
    "# traj = X_measure.detach().cpu().squeeze().numpy()\n",
    "# plt.plot(traj.T[0], traj.T[1], 'b--')\n",
    "\n",
    "# plt.axis('equal')\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "## CREATE TRAINING DATA ##\n",
    "##########################\n",
    "\n",
    "Pu, Pd, R1, R1i = construct_random_mapping(S1, Si1, args) # Get random matrices\n",
    "\n",
    "# Build training dataset\n",
    "train_loader, train_dataset, X_true_all, X_measure_all, t_measure_all = create_train_loader_dynamics(A, S1, Si1, Pu, Pd, R1, R1i, t_vector, sigma_process, sigma_measure, args, args.k_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## PLOT ALL TRAJECTORIES ##\n",
    "###########################\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# for it, (train_data, X_true, X_measure, t_measure_full) in enumerate(train_loader):\n",
    "for it, (inputs, target, X_true, X_measure, t_measure) in enumerate(train_loader):\n",
    "    # Actual trajectory\n",
    "    X_true_plt = X_true.squeeze().detach().cpu().numpy()\n",
    "    plt.plot(X_true_plt.T[0],X_true_plt.T[1],'black')\n",
    "\n",
    "    # Noisy trajectory\n",
    "    traj = X_measure.detach().cpu().squeeze().numpy()\n",
    "    plt.plot(traj.T[0], traj.T[1], 'b--')\n",
    "    \n",
    "    x0 = X_true_plt[:,0].T\n",
    "    plt.scatter(x0[0],x0[1], color='red')\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F6aLjrv8-gS",
    "outputId": "fbf50e25-b410-4406-a22f-4543fe725e59"
   },
   "outputs": [],
   "source": [
    "####################\n",
    "## MODEL SETTINGS ##\n",
    "####################\n",
    "\n",
    "args.max_learned_decay = 1.4 # e/2\n",
    "# args.max_learned_decay = 5.0\n",
    "args.max_fixed_decay = 5.0 # Can be more aggressive\n",
    "\n",
    "# Limits for clamping exponent\n",
    "args.max_exponent = 0\n",
    "args.min_exponent = -30\n",
    "\n",
    "# Key, query, value embedding dimensions are same as input embedding dimension\n",
    "args.d_k = args.d_e\n",
    "args.d_v = args.d_e\n",
    "\n",
    "args.epsilon = 1E-5 # Stability param\n",
    "\n",
    "args.compute_metadata = True # Triggers computing various diagnostics; turned off during training\n",
    "args.compute_pulled_forward_estimates = True # \"Project\" every past state into every future frame; very expensive.\n",
    "\n",
    "if args.d_k != args.d_v and args.sep_params == 0:\n",
    "    print('ERROR: Key and value embedding dimensions must be the same if using shared parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Ablation options ##\n",
    "######################\n",
    "args.causal = True\n",
    "args.sep_params = False # Use separate params for keys and values?\n",
    "args.lambda_real_zero = 0 # Zero out real part of eigenvalues?\n",
    "args.use_full_residual_norm = 1 # Use the full |R|^2 metric and rational robust weight (1) or dot product / exponential weight (0)?\n",
    "args.use_robust_weight = True # Use rational weight rather than softmax\n",
    "args.additive_bias_type = 1 # (Additive bias: 0 for zero; 1 for DLE; 2 for linear)\n",
    "args.multiplicative_bias_type = 1 # (Multiplicative bias: 0 for constant; 1 for DLE; 2 for linear)\n",
    "# args.t_shift = None # Default\n",
    "# args.t_shift = 1.0\n",
    "# args.t_shift = args.k_step\n",
    "args.learn_t_shift = True\n",
    "if args.learn_t_shift == True:\n",
    "    args.t_shift = None\n",
    "args.learn_rotations = True # Learned rotations (True), or fixed as in RoPE (False)?\n",
    "args.learn_decay = True # Learned decay (True), or fixed (False)?\n",
    "args.rotate_values = True # Rotate/unrotate values?\n",
    "args.zero_process_noise = False # Zero process noise (sigma^2)?\n",
    "args.zero_key_measurement_noise = False # Zero key measurement noise (eta^2)?\n",
    "args.use_total_precision_gate = 1 # Use total-precision gating? (0 = No gate, 1 = precision gate, 2 = learned gate)\n",
    "args.use_inner_residual = True # Include a residual connection BEFORE output projection?\n",
    "args.use_outer_residual = True # Include a residual connection AFTER output projection?\n",
    "args.use_complex_input_norm = 0 # Use complex-valued RMS Norm AFTER input projection for query/key/value (1), complex-valued RMS Norm AFTER input projection only for query/key (2), or None (0)?\n",
    "args.use_complex_output_norm = False # Use complex-valued RMS Norm BEFORE output projection?\n",
    "args.use_real_input_norm = True # Use real-valued RMS Norm BEFORE input projection?\n",
    "args.use_real_output_norm = True # Use real-valued RMS Norm AFTER output projection?\n",
    "args.add_gaussian_noise = False # Add Gaussian noise to final token? (for test-time sampling)\n",
    "args.use_complex_conj_constraint = True # Eigenvalues must appear in complex conjugate pairs to ensure A is real\n",
    "args.use_colored_prior = False\n",
    "# args.allow_BM_branch = True # Allow separate branch for Brownian motion? (only used when learning decay)\n",
    "args.damping = 0.005\n",
    "args.use_ss_process_noise = True\n",
    "args.scale_decay_by_time_interval = False\n",
    "args.zero_rotations = False\n",
    "args.use_SC_RoPE = False\n",
    "args.use_log_linear_decay = False\n",
    "# -----------------------------\n",
    "args.use_rope = True\n",
    "args.use_alibi = False\n",
    "\n",
    "# args.learnable_rope = True\n",
    "\n",
    "args.use_relative_decay_vanilla = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.t_equal == False:\n",
    "    print('Note: t_equal == False path is broken.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## DEFINE MODEL ##\n",
    "##################\n",
    "\n",
    "# Settings\n",
    "args.n_heads = 1 # Number of heads\n",
    "args.d_k_total = args.d_e # Total query-key dim across all heads\n",
    "args.d_v_total = args.d_e # Total value dim across all heads\n",
    "# args.num_blocks = 3\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Standard Attention\n",
    "\n",
    "# model = SelfAttentionBlock(input_dim=args.d_e, qkv_dim=args.d_e, num_heads=args.n_heads, args=args)\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "# Multihead Isotropic RFA\n",
    "\n",
    "model = RFA_Block(args, args.n_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Standard Transformer\n",
    "\n",
    "# args.num_blocks = 6\n",
    "# model = TransformerNetwork(args.d_e, args.d_v*2, args.d_v*4, args.n_heads, args, num_blocks=args.num_blocks)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # RFA Transformer\n",
    "\n",
    "# args.num_blocks = 6\n",
    "# model = RFATransformerNetwork(args=args, num_blocks=args.num_blocks, n_heads=args.n_heads, input_dim=args.d_e, query_key_dim_total=args.d_k, value_dim_total=args.d_v, hidden_dim = 4*args.d_v, Norm=nn.LayerNorm)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "model.to(args.device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "params_list = list(model.parameters()) # Parameters list\n",
    "\n",
    "print('Total parameter count:', count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## TRAINING SETUP ##\n",
    "####################\n",
    "\n",
    "criterion = nn.MSELoss() # Loss\n",
    "\n",
    "args.batch_size = 32 # Batch size\n",
    "args.num_epochs = int(4000 / args.num_batch) # Number of epochs\n",
    "args.num_its = int(args.num_samp/args.batch_size) # Number of iterations in an epoch\n",
    "\n",
    "args.save_model = False\n",
    "\n",
    "args.save_epochs = int(args.num_epochs/5) # Intervals of epochs to save model\n",
    "# args.show_example_epochs = int(args.num_epochs/10) # Number of epochs between displaying results\n",
    "args.show_example_epochs = 5\n",
    "args.n_example = 5 # Plot state estimates at n_example data points\n",
    "\n",
    "#####################\n",
    "\n",
    "# Create folders for model weights, and loss history\n",
    "\n",
    "try:\n",
    "    root_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    root_path = os.getcwd()\n",
    "\n",
    "saved_models_path = os.path.join(root_path, 'saved_models\\\\dynamics\\\\')\n",
    "model_name = str(model.__class__.__name__)\n",
    "date = str(datetime.datetime.today()).split()[0]\n",
    "# model_path = saved_models_path + model_name + '__' + date + '\\\\'\n",
    "model_path = os.path.join(saved_models_path, f\"{model_name}__{date}\")\n",
    "\n",
    "model_weight_path = model_path + 'model_weights\\\\'\n",
    "model_tensor_path = model_path + 'info_tensors\\\\'\n",
    "model_images_path = model_path + 'train_imgs\\\\'\n",
    "\n",
    "try:\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.makedirs(model_weight_path, exist_ok=True)\n",
    "    os.makedirs(model_tensor_path, exist_ok=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# args.lr = 1E-2 # Learning rate\n",
    "# optimizer = torch.optim.Adam(params_list, lr=args.lr, betas=(0.9, 0.999)) # Optimizer\n",
    "\n",
    "# Separate the \"Physics\" from the \"Features\"\n",
    "sde_params = [p for n, p in model.named_parameters() if any(k in n for k in ['mu_', 'sigma_', 'eta_', 'gamma_'])]\n",
    "feature_params = [p for n, p in model.named_parameters() if not any(k in n for k in ['mu_', 'sigma_', 'eta_', 'gamma_'])]\n",
    "\n",
    "feature_lr = 1e-2\n",
    "# feature_lr = 1e-3\n",
    "sde_lr = feature_lr/2\n",
    "\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': feature_params, 'lr': feature_lr},\n",
    "#     {'params': sde_params, 'lr': sde_lr}  # slower to prevent spikes\n",
    "# ])\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    # Standard Weights (with momentum)\n",
    "    {'params': feature_params, 'lr': feature_lr, 'betas': (0.9, 0.999)},\n",
    "    \n",
    "    # Decay Params (Lower learning rate, NO momentum, higher epsilon)\n",
    "    {\n",
    "        'params': sde_params, \n",
    "        'lr': sde_lr,          # Lower learning rate\n",
    "        'betas': (0.0, 0.999), # First beta=0 kills momentum\n",
    "        'eps': 1e-7            # Higher eps prevents division-by-zero spikes\n",
    "    }\n",
    "])\n",
    "\n",
    "########################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "\n",
    "# scheduler = None\n",
    "\n",
    "################################\n",
    "\n",
    "# Cosine annealing\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "#     max_lr=args.lr,        # Target learning rate\n",
    "    max_lr = [feature_lr, sde_lr],\n",
    "    epochs=args.num_epochs, \n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.05,          # Spend 5% of training time warming up\n",
    "    anneal_strategy='cos',   # Use cosine decay\n",
    "    div_factor=25.0,         # Start LR is args.lr / 25\n",
    "    final_div_factor=1000.0  # Final LR is args.lr / 1000\n",
    ")\n",
    "\n",
    "################################\n",
    "\n",
    "# # Multi-stage learning rate schedule (for SDE param training)\n",
    "# 1. Warmup: Stabilizes initial gradients. Projections (W_q, W_k, W_v) must first align \n",
    "#    embeddings into a coherent latent space.\n",
    "# 2. Simmer: The \"Co-adaptation\" phase. SDE parameters (especially decay mu) rarely \n",
    "#    converge until the feature coordinate system is stable. Once the loss curve \n",
    "#    levels off, the SDE params begin to capture the structural temporal dependencies.\n",
    "#    While SDE params contribute marginally to training loss, they are helpful for model\n",
    "#    robustness and generalization (out-of-distribution performance).\n",
    "# 3. Freeze: High-precision refinement.\n",
    "\n",
    "# def get_schedules(optimizer, warmup_steps, milestone, total_steps, feature_lr, sde_lr):\n",
    "    \n",
    "#     # Define the multipliers relative to the initial LR of each group\n",
    "#     # Group 0: Features, Group 1: SDE\n",
    "    \n",
    "#     def simmer_lambda(step):\n",
    "#         # Cosine from 1.0 down to 0.1\n",
    "#         if step < warmup_steps: return 1.0 # Should be handled by SequentialLR\n",
    "#         t = (step - warmup_steps) / (milestone - warmup_steps)\n",
    "#         return 0.1 + 0.9 * (1 + math.cos(math.pi * t)) / 2\n",
    "\n",
    "#     def freeze_lambda(step):\n",
    "#         # Cosine from 0.1 down to 0.001\n",
    "#         t = (step - milestone) / (total_steps - milestone)\n",
    "#         return 0.001 + 0.099 * (1 + math.cos(math.pi * t)) / 2\n",
    "\n",
    "#     # 1. Warmup\n",
    "#     s1 = torch.optim.lr_scheduler.LinearLR(\n",
    "#         optimizer, start_factor=1/25.0, end_factor=1.0, total_iters=warmup_steps\n",
    "#     )\n",
    "\n",
    "#     # 2. Simmer\n",
    "#     s2 = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=simmer_lambda)\n",
    "\n",
    "#     # 3. Freeze\n",
    "#     s3 = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=freeze_lambda)\n",
    "\n",
    "#     return torch.optim.lr_scheduler.SequentialLR(\n",
    "#         optimizer, \n",
    "#         schedulers=[s1, s2, s3], \n",
    "#         milestones=[warmup_steps, milestone]\n",
    "#     )\n",
    "\n",
    "# # Define steps\n",
    "# steps_per_epoch = len(train_loader)\n",
    "# total_steps = steps_per_epoch * args.num_epochs\n",
    "# warmup_steps = int(0.10 * total_steps)\n",
    "# milestone = int(0.70 * total_steps)\n",
    "# final_steps = total_steps - milestone\n",
    "\n",
    "# scheduler = get_schedules(optimizer, warmup_steps, milestone, total_steps, feature_lr, sde_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Initialize model to correct values (for testing) ######\n",
    "# ##############################################################\n",
    "\n",
    "# if isinstance(model, MultiheadIsotropicRFA_1layer):\n",
    "#     module = model.layers[0]\n",
    "#     initialize_to_correct_model(module, D1, S1, Si1, sigma_process, sigma_measure, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  Load in model weights ######\n",
    "####################################\n",
    "\n",
    "# epoch = 410\n",
    "# model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "\n",
    "# try:\n",
    "#     model.load_state_dict(torch.load(model_weight_path_full, weights_only=True))\n",
    "#     print('Model loaded.')\n",
    "# except:\n",
    "#     print('Model loading failed.')\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### RFA TRAINING LOOP ###########################\n",
    "#########################################################################\n",
    "\n",
    "print(f\"Starting training on {args.device}...\")\n",
    "\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'mu': [],     # Each entry will be [n_heads]\n",
    "    'sigma': [],\n",
    "    'sigma_tilde': [],\n",
    "    'eta': [],\n",
    "    'gamma': [],\n",
    "    'tau': [],\n",
    "    'nu_over_d': [],\n",
    "    'gs_mean': [] # Mean gate value per iteration\n",
    "}\n",
    "\n",
    "# Train for num_epochs:\n",
    "for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "\n",
    "    # Train for single epoch\n",
    "    output_dict, history = single_epoch_rfa_dynamics(model, train_loader, history, optimizer, criterion, params_list, args, t_equal=args.t_equal, t_shift=args.t_shift, causal=args.causal, scheduler=scheduler)\n",
    "\n",
    "    # Visualize results so far:\n",
    "    if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            visualize_results(\n",
    "            model, train_dataset, history, \n",
    "            R1, R1i, Pu, Pd, A, epoch, model_images_path, args, t_equal=args.t_equal, t_shift=args.t_shift, causal=args.causal,\n",
    "            plot_losses_flag=False, plot_log_losses_flag=True, plot_traj_flag=True,\n",
    "            plot_pulled_forward_estimates_flag=True, plot_last_attn_mat_flag=False,\n",
    "            plot_total_precision_flag=False, plot_attn_prior_flag=False, plot_eigenvals_flag=True,\n",
    "            plot_decay_per_epoch=False, plot_noise_params_by_type=False, plot_noise_params_by_head=False,\n",
    "            plot_tau_and_nu_flag=False, plot_gates_per_epoch=False)\n",
    "        model.train()\n",
    "    \n",
    "    # Save model and training history\n",
    "    if args.save_model == True and np.mod(epoch + 1, args.save_epochs) == 0:\n",
    "            # Save Model Weights\n",
    "            model_weight_path_full = os.path.join(model_weight_path, f'weights_epoch_{epoch}.pth')\n",
    "            torch.save(model.state_dict(), model_weight_path_full)\n",
    "\n",
    "            # Prepare Tensors/History for saving\n",
    "            model_tensor_path_full = os.path.join(model_tensor_path, f'tensors_epoch_{epoch}.pth')\n",
    "\n",
    "            tensors_to_save = {\n",
    "                'history': history,                    # All scalar tracking (mu, sigma, loss, etc.)\n",
    "                'attn_mat': output_dict['attn_mat'],   # Latest attention matrix\n",
    "                'eigenvals': output_dict['eigenvals'], # Latest eigenvalues\n",
    "                'args': args                           # Save args so we know the config later\n",
    "            }\n",
    "\n",
    "            torch.save(tensors_to_save, model_tensor_path_full)\n",
    "            print(f\"Checkpoint saved: Epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JUMP\n",
    "\n",
    "# Final vizualization\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    visualize_results(\n",
    "            model, train_dataset, history, \n",
    "            R1, R1i, Pu, Pd, A, epoch, model_images_path, args, t_equal=args.t_equal, t_shift=args.t_shift, causal=args.causal,\n",
    "            plot_losses_flag=False, plot_log_losses_flag=True, plot_traj_flag=True,\n",
    "            plot_pulled_forward_estimates_flag=True, plot_last_attn_mat_flag=True,\n",
    "            plot_total_precision_flag=False, plot_attn_prior_flag=True, plot_eigenvals_flag=True,\n",
    "            plot_decay_per_epoch=True, plot_noise_params_by_type=True, plot_noise_params_by_head=False,\n",
    "            plot_tau_and_nu_flag=True, plot_gates_per_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test model\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     # Get prediction for random choice of input\n",
    "#     rand_idx = np.random.choice(args.num_samp)\n",
    "#     train_data, X_true, X_measure, t_measure = train_dataset.__getitem__(rand_idx)\n",
    "\n",
    "#     inputs = train_data.unsqueeze(0)[:, :-1]\n",
    "    \n",
    "#     print(inputs.size())\n",
    "\n",
    "#     out, output_dict = model.forward(inputs)\n",
    "    \n",
    "#     est = output_dict['est_latent']\n",
    "#     attn_mat = output_dict['attn_mat']\n",
    "#     A_prior = output_dict['A_prior']\n",
    "#     x_hat = output_dict['x_hat']\n",
    "#     lambda_h = output_dict['epoch_lambdas']\n",
    "#     P_tot = output_dict['P_tot']\n",
    "#     gate = output_dict['gate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save Model\n",
    "# model_weight_path_full = model_weight_path + 'weights_epoch_' + str(epoch)\n",
    "# model_tensor_path_full = model_tensor_path + 'tensors_epoch_' + str(epoch)\n",
    "# torch.save(model.state_dict(), model_weight_path_full)\n",
    "\n",
    "# tensors_to_save = {\n",
    "# 'epoch_losses': epoch_losses,\n",
    "# 'A_hat_complex': A_hat_complex,\n",
    "# 'epoch_lambdas': epoch_lambdas\n",
    "# }\n",
    "# torch.save(tensors_to_save, model_tensor_path_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################\n",
    "# ##### LOOP FOR STANDARD ATTENTION #######\n",
    "\n",
    "# all_losses = [] # Global list to store every iteration\n",
    "\n",
    "# for epoch in tqdm(np.arange(args.num_epochs), desc=\"Training progress...\"):\n",
    "\n",
    "#     epoch_losses = single_epoch_attn_dynamics(model, train_loader, optimizer, criterion, params_list, args)\n",
    "\n",
    "#     # Append the epoch's iterations to our global history\n",
    "#     all_losses.extend(epoch_losses)\n",
    "    \n",
    "#     # Visualize results so far:\n",
    "#     if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "#         visualize_results_attn(model, train_dataset, all_losses, R1, R1i, Pu, Pd, A, epoch, args)\n",
    "# #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if np.mod(epoch+1,args.show_example_epochs) == 0:\n",
    "#     visualize_results_attn(model, train_dataset, all_losses, R1, R1i, Pu, Pd, A, epoch, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Test autoregressive sampling ##\n",
    "##################################\n",
    "\n",
    "args.add_gaussian_noise = 1\n",
    "args.max_gen_len = 100\n",
    "\n",
    "# for it, (train_data, _, _, _) in enumerate(train_loader):\n",
    "#     start_seq = train_data[:, :-1]\n",
    "for it, (inputs, _, _, _, t_measure) in enumerate(train_loader):\n",
    "    start_seq = inputs\n",
    "    break\n",
    "\n",
    "total_seq, new_seq, precisions = autoregressive_sample(model, start_seq, args.max_gen_len, t_measure, t_shift=args.t_shift, t_equal=args.t_equal, causal=True)\n",
    "\n",
    "traj_tot = torch.matmul(R1i,total_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "X_tot = torch.matmul(Pd,traj_tot) # Map back to lower dim\n",
    "\n",
    "traj_new = torch.matmul(R1i,new_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "X_new = torch.matmul(Pd,traj_new) # Map back to lower dim\n",
    "\n",
    "# Predicted trajectory\n",
    "X_tot_plt = X_tot.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "plt.plot(X_tot_plt.T[0], X_tot_plt.T[1], 'r--', label='Predicted') # Added label\n",
    "\n",
    "X_new_plt = X_new.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "plt.plot(X_new_plt.T[0], X_new_plt.T[1], 'b', label='Predicted') # Added label\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack and normalize collected precisions\n",
    "p_tensor = torch.stack(precisions, dim=1) # [B, max_gen_len]\n",
    "# Normalize relative to the sequence's maximum confidence for the heatmap\n",
    "p_min, p_max = p_tensor.min(), p_tensor.max()\n",
    "p_normalized = (p_tensor - p_min) / (p_max - p_min + 1e-8)\n",
    "plt.plot(p_normalized[:,-1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = model.layers[0]\n",
    "tau = F.softplus(self.tau_param) + self.args.epsilon # Softmax temperature\n",
    "print(tau)\n",
    "d = self.d_k_head\n",
    "nu = d * F.softplus(self.nu_param) + 2.0 + self.args.epsilon\n",
    "print(nu/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F.softplus(model.layers[0].t_shift_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recursive_state_update(model, start_seq, num_steps, t_measure=None, t_shift=None):\n",
    "#     \"\"\"\n",
    "#     Iteratively updates a fixed-size trajectory.\n",
    "#     Input 'start_seq' is transformed into 'out' n times.\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     current_state = start_seq\n",
    "    \n",
    "#     traj_start = torch.matmul(R1i,start_seq.unsqueeze(-1)) # Reverse random mapping\n",
    "#     X_start = torch.matmul(Pd,traj_start) # Map back to lower dim\n",
    "#     X_start_plt = X_start.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "#     plt.plot(X_start_plt.T[0], X_start_plt.T[1], 'r--', label='Input') \n",
    "\n",
    "#     # We collect the 'state' at each iteration to see how the trajectory evolves\n",
    "#     # but the sequence length (L) never changes.\n",
    "#     evolution_history = [] \n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(num_steps):\n",
    "#             # The model treats the current window as a set of noisy observations\n",
    "#             # and filters them through the SDE prior.\n",
    "#             out, output_dict = model(current_state, t_measure=t_measure, t_shift=t_shift)\n",
    "            \n",
    "#             # RECURSION: The entire output block is the next input block\n",
    "#             current_state = out \n",
    "            \n",
    "#             evolution_history.append(current_state)\n",
    "            \n",
    "#             traj_filt = torch.matmul(R1i,current_state.unsqueeze(-1)) # Reverse random mapping\n",
    "#             X_filt = torch.matmul(Pd,traj_filt) # Map back to lower dim\n",
    "#             X_filt_plt = X_filt.squeeze(0)[0].detach().cpu().squeeze().numpy()\n",
    "#             plt.plot(X_filt_plt.T[0], X_filt_plt.T[1]) \n",
    "\n",
    "#         plt.grid()\n",
    "#         plt.show()\n",
    "\n",
    "#     return current_state, evolution_history\n",
    "\n",
    "\n",
    "# args.add_gaussian_noise = 1\n",
    "# num_steps = 30\n",
    "\n",
    "# for it, (inputs, _, _, _, t_measure) in enumerate(train_loader):\n",
    "#     start_seq = inputs\n",
    "#     break\n",
    "\n",
    "# seq_filt, evolution_history = recursive_state_update(model, start_seq, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
