{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6d514e",
   "metadata": {},
   "source": [
    "## Robust Filter Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045d0d7",
   "metadata": {},
   "source": [
    "### Training on Wikitext-103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0302ab5d",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import patches\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.rc('font', size=20)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import transformers\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # Loading bar\n",
    "import glob\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf03acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import complex_matmul, apply_interleaved_rope\n",
    "from utils import count_parameters, get_layers, seed_everything\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isotropic_rfa import get_safe_exp_tot, compute_covariance_matrix, compute_covariance_matrix_LHopital\n",
    "from isotropic_rfa import compute_covariance_matrix_spectral_full, compute_covariance_matrix_residual_diffusion\n",
    "from isotropic_rfa import compute_exp_kernel_isotropic, compute_residual_norm_isotropic\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import resolve_multihead_dims, autoregressive_sample\n",
    "from model import init_complexlinear, init_complex_matrix, initialize_linear_layers\n",
    "from model import init_rope, init_decay_per_head, init_linear_bias_slopes\n",
    "from model import apply_weight_masks\n",
    "from model import ComplexLinearLayer, ComplexLinearHermitianLayer, ComplextoRealLinearLayer\n",
    "from model import ComplexRMSNorm\n",
    "from model import MultiHeadAttentionLayer, MultiheadIsotropicRFA\n",
    "from model import TransformerBlock, TransformerNetwork\n",
    "from model import SelfAttentionBlock, RFA_Block\n",
    "from model import RFATransformerBlock, RFATransformerNetwork\n",
    "from model import LanguageModel\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63456b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import plot_trajectory, compute_state_matrix, plot_state_matrix, visualize_results\n",
    "from visualization import visualize_results_attn, _get_visual_modules, visualize_rfa_lm\n",
    "from visualization import plot_training_progress_lm\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import single_epoch_rfa_lm, single_epoch_standard_lm\n",
    "from training import hook_fn\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf79216",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('DA')\n",
    "parser.add_argument('--gpu', type=int, default=0) # (Default: 0)\n",
    "args = parser.parse_args(args=[])\n",
    "args.device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(args.device)\n",
    "    \n",
    "seed_everything(seed=2025) # Set random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82f718",
   "metadata": {},
   "source": [
    "### Load Wikitext-103 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c911bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikitext data files\n",
    "data_files = {\n",
    "    \"train\": \"datasets/wikitext-103/train-*.parquet\",\n",
    "    \"validation\": \"datasets/wikitext-103/validation-*.parquet\",\n",
    "    \"test\": \"datasets/wikitext-103/test-*.parquet\"\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "raw_datasets = load_dataset(\"parquet\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c98505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") \n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\n",
    "    \"./gpt2_tokenizer/\",\n",
    "    local_files_only=True  # This GUARANTEES it won't try to use the internet\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and Chunk\n",
    "def tokenize_function(examples, tokenizer=None):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4, \n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group into blocks\n",
    "\n",
    "def group_texts(examples, block_size):\n",
    "    from itertools import chain\n",
    "    # Flatten all the token lists into one long list (Efficiently)\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    \n",
    "    # Get the total number of tokens\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Round down to the nearest multiple of block_size\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        \n",
    "    # Chop into fixed-size chunks\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Create the labels (same as inputs for Causal LM)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "block_size = 512\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    fn_kwargs={\"block_size\": 512}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a70328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single example\n",
    "sample = lm_datasets[\"train\"][0]\n",
    "\n",
    "print(f\"Dataset size: {len(lm_datasets['train'])} blocks\")\n",
    "print(f\"Sequence Length: {len(sample['input_ids'])}\")\n",
    "print(f\"Features: {lm_datasets['train'].column_names}\")\n",
    "\n",
    "# Verify label alignment (should be identical before the model-side shift)\n",
    "is_aligned = sample['input_ids'] == sample['labels']\n",
    "print(f\"Labels perfectly aligned with inputs: {is_aligned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8876a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the first block\n",
    "decoded_text = tokenizer.decode(sample['input_ids'])\n",
    "\n",
    "print(\"--- SAMPLE DATA START ---\")\n",
    "print(decoded_text[:500]) # Print first 500 characters\n",
    "print(\"--- SAMPLE DATA END ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af68cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample 1000 tokens from the first few blocks\n",
    "# all_tokens = []\n",
    "# for i in range(5):\n",
    "#     all_tokens.extend(lm_datasets[\"train\"][i][\"input_ids\"])\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.hist(all_tokens, bins=100, color='skyblue', edgecolor='black')\n",
    "# plt.title(\"Token ID Distribution (WikiText-103)\")\n",
    "# plt.xlabel(\"Token ID\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.grid(axis='y', alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5241bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets[\"train\"]\n",
    "# Dataset({\n",
    "#     features: ['input_ids', 'attention_mask', 'labels'],\n",
    "#     num_rows: 229206\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e3558",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## MODEL SETTINGS ##\n",
    "####################\n",
    "\n",
    "args.batch_size = 16 # Batch size\n",
    "args.vocab_size = 50257\n",
    "args.d_e = 256\n",
    "args.seq_len = 512\n",
    "args.n_heads = 8\n",
    "args.num_blocks = 6\n",
    "args.d_k_total = args.d_e # Total query-key dim across all heads\n",
    "args.d_v_total = args.d_e # Total value dim across all heads\n",
    "# args.num_blocks = 3\n",
    "\n",
    "# args.max_learned_decay = 1.4 # e/2\n",
    "args.max_learned_decay = 5.0\n",
    "args.max_fixed_decay = 5.0 # Can be more aggressive\n",
    "\n",
    "# Limits for clamping exponent\n",
    "args.max_exponent = 0\n",
    "args.min_exponent = -10\n",
    "\n",
    "args.epsilon = 1E-5 # Stability param\n",
    "\n",
    "args.compute_metadata = False # Triggers computing various diagnostics; turned off during training\n",
    "args.compute_pulled_forward_estimates = False # \"Project\" every past state into every future frame; very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72150bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## DEFAULT ABLATION OPTIONS ##\n",
    "##############################\n",
    "args.causal = True\n",
    "args.t_equal = True # Equal time intervals?\n",
    "args.sep_params = False # Use separate params for keys and values?\n",
    "args.lambda_real_zero = False # Zero out real part of eigenvalues?\n",
    "args.use_full_residual_norm = 1 # Use the full |R|^2 metric?\n",
    "args.use_robust_weight = True # Use rational weight rather than softmax\n",
    "args.additive_bias_type = 1 # (Additive bias: 0 for zero; 1 for DLE; 2 for linear)\n",
    "args.multiplicative_bias_type = 1 # (Multiplicative bias: 0 for constant; 1 for DLE; 2 for linear)\n",
    "args.t_shift = None # Default\n",
    "args.learn_t_shift = True\n",
    "if args.learn_t_shift == True:\n",
    "    args.t_shift = None\n",
    "args.learn_rotations = False # Learned rotations (True), or fixed as in RoPE (False)?\n",
    "args.learn_decay = False # Learned decay (True), or fixed (False)?\n",
    "args.rotate_values = True # Rotate/unrotate values?\n",
    "args.zero_process_noise = False # Zero process noise (sigma^2)?\n",
    "args.zero_key_measurement_noise = False # Zero key measurement noise (eta^2)?\n",
    "args.use_total_precision_gate = 1 # Use total-precision gating? (0 = No gate, 1 = precision gate, 2 = learned gate)\n",
    "args.use_inner_residual = False # Include a residual connection BEFORE output projection?\n",
    "args.use_outer_residual = True # Include a residual connection AFTER output projection?\n",
    "args.use_complex_input_norm = 0 # Use complex-valued RMS Norm AFTER input projection for query/key/value (1), complex-valued RMS Norm AFTER input projection only for query/key (2), or None (0)?\n",
    "args.use_complex_output_norm = False # Use complex-valued RMS Norm BEFORE output projection?\n",
    "args.use_real_input_norm = True # Use real-valued RMS Norm BEFORE input projection?\n",
    "args.use_real_output_norm = True # Use real-valued RMS Norm AFTER output projection?\n",
    "args.add_gaussian_noise = False # Add Gaussian noise to final token? (for test-time sampling)\n",
    "args.use_complex_conj_constraint = True # Eigenvalues must appear in complex conjugate pairs to ensure A is real\n",
    "args.use_colored_prior = False\n",
    "# args.allow_BM_branch = True # Allow separate branch for Brownian motion? (only used when learning decay)\n",
    "args.scale_decay_by_time_interval = True\n",
    "args.zero_rotations = False\n",
    "args.use_ss_process_noise = False\n",
    "args.damping = 0.05\n",
    "\n",
    "args.use_rope = True\n",
    "args.use_alibi = False\n",
    "# args.use_xpos = False\n",
    "\n",
    "args.use_SC_RoPE = False\n",
    "args.use_log_linear_decay = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee497f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABLATIONS ##\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # BASELINES:\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # B1: Standard Transformer + RoPE Baseline\n",
    "# #     --- Dot-Product Similarity; current SOTA positional encoding baseline.\n",
    "# #     Applies d --> 2d --> d attention projections for fair comparison.\n",
    "# # Uses separate backbone and training looping\n",
    "\n",
    "# ablation_name = 'B1'\n",
    "# args.use_rope = True\n",
    "# args.use_alibi = False\n",
    "# args.use_relative_decay_vanilla = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # B2: Standard Transformer + ALiBi Baseline\n",
    "# #     Applies d --> 2d --> d attention projections for fair comparison.\n",
    "# # Uses separate backbone and training looping\n",
    "\n",
    "# ablation_name = 'B2'\n",
    "# args.use_rope = False\n",
    "# args.use_alibi = True\n",
    "# args.use_relative_decay_vanilla = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # B3 (RoPE + decay):\n",
    "# ablation_name = 'B3'\n",
    "# args.use_rope = True\n",
    "# args.use_alibi = False\n",
    "# args.use_SC_RoPE = False\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.use_relative_decay_vanilla = True\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # B4 (SC-RoPE):\n",
    "# ablation_name = 'B4'\n",
    "# args.use_rope = False\n",
    "# args.use_alibi = False\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.use_relative_decay_vanilla = True\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# # MAIN MODELS:\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M1: Isotropic RFA\n",
    "\n",
    "# # Default settings\n",
    "# ablation_name = 'M1'\n",
    "# args.use_log_linear_decay = False\n",
    "# args.damping = 0.05\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # # M1.1: Isotropic RFA w/o DLE Prior\n",
    "\n",
    "# # Default settings\n",
    "# ablation_name = 'M1.1'\n",
    "# args.use_log_linear_decay = False\n",
    "# args.damping = 0.05\n",
    "# args.zero_process_noise = True\n",
    "# args.zero_key_measurement_noise = True\n",
    "# args.additive_bias_type = 0\n",
    "# args.multiplicative_bias_type = 0\n",
    "# args.use_log_linear_decay = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M1.2: Isotropic RFA w/o robust weight\n",
    "\n",
    "# # Default settings\n",
    "# ablation_name = 'M1.2'\n",
    "# args.use_log_linear_decay = False\n",
    "# args.damping = 0.05\n",
    "# args.use_robust_weight = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M2: Spectrally-Coupled RFA\n",
    "# # # --- M1, but we partition the angular frequencies so that the fast frequencies\n",
    "# # # are coupled with fast decays and vice versa\n",
    "\n",
    "# ablation_name = 'M2'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.005\n",
    "# args.use_log_linear_decay = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M2.1 (M2 + no robust weight)\n",
    "# ablation_name = 'M2.1'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.use_robust_weight = False\n",
    "# args.use_log_linear_decay = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M2.2: Spectrally-Coupled RFA, w/o DLE Prior\n",
    "# # # --- M2, but we ablate the DLE\n",
    "\n",
    "# ablation_name = 'M2.2'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.zero_process_noise = True\n",
    "# args.zero_key_measurement_noise = True\n",
    "# args.additive_bias_type = 0\n",
    "# args.multiplicative_bias_type = 0\n",
    "# args.use_log_linear_decay = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# M2.3: (M2 + ablate out only multiplicative gate but keep additive term)\n",
    "\n",
    "# ablation_name = 'M2.3'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.additive_bias_type = 1 # Keep additive term\n",
    "# args.multiplicative_bias_type = 0 # Set multiplicative term to constant\n",
    "# args.use_log_linear_decay = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M2.4: (M2 + No Value rotations)\n",
    "\n",
    "# ablation_name = 'M2.4'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.use_log_linear_decay = False\n",
    "# args.rotate_values = False\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M2.5: (M2 + No Rotations)\n",
    "# ablation_name = 'M2.5'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.05\n",
    "# args.use_log_linear_decay = False\n",
    "# args.zero_rotations = True\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# # M2.6: (Unitary, zero noise Limit)\n",
    "# ablation_name = 'M2.6'\n",
    "# args.use_SC_RoPE = True\n",
    "# args.scale_decay_by_time_interval = False\n",
    "# args.damping = 0.0\n",
    "# args.use_log_linear_decay = False\n",
    "\n",
    "# args.use_robust_weight = False\n",
    "# args.use_full_residual_norm = 0\n",
    "# args.lambda_real_zero = True\n",
    "# args.zero_process_noise = True\n",
    "# args.zero_key_measurement_noise = True\n",
    "# args.additive_bias_type = 0\n",
    "# args.multiplicative_bias_type = 0\n",
    "# args.max_fixed_decay = 0.0 # Zero out decay\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# M2.7: Spectrally-Coupled RFA + Total Confidence Gate\n",
    "\n",
    "ablation_name = 'M2.7'\n",
    "args.use_SC_RoPE = True\n",
    "args.scale_decay_by_time_interval = False\n",
    "args.damping = 0.05\n",
    "args.use_log_linear_decay = False\n",
    "args.use_inner_residual = True\n",
    "args.use_total_precision_gate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ablation_name)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ef9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## DEFINE BACKBONE ##\n",
    "#####################\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # Standard Attention\n",
    "\n",
    "# backbone = SelfAttentionBlock(input_dim=args.d_e, qkv_dim=args.d_e, num_heads=args.n_heads, args=args)\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "# # Multihead Isotropic RFA\n",
    "\n",
    "# backbone = RFA_Block(args, args.n_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# # # Standard Transformer\n",
    "\n",
    "# backbone = TransformerNetwork(args.d_e, args.d_e*2, args.d_e*4, args.n_heads, args, num_blocks=args.num_blocks, Norm=nn.LayerNorm)\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# RFA Transformer\n",
    "\n",
    "backbone = RFATransformerNetwork(args=args, num_blocks=args.num_blocks, n_heads=args.n_heads, input_dim=args.d_e, query_key_dim_total=args.d_k_total, value_dim_total=args.d_v_total, hidden_dim = 4*args.d_v_total, Norm=nn.LayerNorm)\n",
    "\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526332ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap it to create the Language Model\n",
    "model = LanguageModel(\n",
    "    backbone=backbone, \n",
    "    vocab_size=args.vocab_size, \n",
    "    embed_dim=args.d_e\n",
    ").to(args.device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "params_list = list(model.parameters()) # Parameters list\n",
    "\n",
    "print('Total parameter count:', count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77a0d3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04317d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Data Collator\n",
    "# mlm=False is crucial: it tells the collator we are doing Causal LM, \n",
    "# not Masked LM (like BERT). It will ensure labels are handled correctly.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    lm_datasets[\"validation\"],\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    lm_datasets[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Quick Test of the first batch\n",
    "batch = next(iter(train_dataloader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e844296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DEBUG BY TESTING ON SUBSET OF DATASET ##\n",
    "# ###########################################\n",
    "\n",
    "# # Define how many batches we want to test (e.g., 50 batches)\n",
    "# # If batch_size is 16, 1000 samples = ~62 batches\n",
    "# num_debug_samples = 1000 \n",
    "\n",
    "# train_dataset = train_dataloader.dataset\n",
    "# val_dataset = eval_dataloader.dataset \n",
    "\n",
    "# # Create subsets\n",
    "# train_subset = Subset(train_dataset, range(min(num_debug_samples, len(train_dataset))))\n",
    "# val_subset = Subset(val_dataset, range(min(num_debug_samples // 5, len(val_dataset))))\n",
    "\n",
    "# # Create temporary debug loaders\n",
    "# debug_train_loader = torch.utils.data.DataLoader(\n",
    "#     train_subset, \n",
    "#     batch_size=args.batch_size, \n",
    "#     shuffle=True,\n",
    "#     collate_fn=train_dataloader.collate_fn\n",
    "# )\n",
    "\n",
    "# debug_val_loader = torch.utils.data.DataLoader(\n",
    "#     val_subset, \n",
    "#     batch_size=args.batch_size, \n",
    "#     shuffle=False,\n",
    "#     collate_fn=eval_dataloader.collate_fn\n",
    "# )\n",
    "\n",
    "# train_dataloader = debug_train_loader\n",
    "# eval_dataloader = debug_val_loader\n",
    "\n",
    "# print(f\"Debug Mode: Training on {len(debug_train_loader)} batches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## TRAINING SETUP ##\n",
    "####################\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss\n",
    "\n",
    "args.num_epochs = 15 # Number of epochs\n",
    "print('Num epochs:', args.num_epochs)\n",
    "\n",
    "args.save_model = True\n",
    "\n",
    "args.save_epochs = 1 # Intervals of epochs to save model\n",
    "args.show_example_epochs = 1 # Number of epochs between displaying results\n",
    "\n",
    "#####################\n",
    "\n",
    "# Create folders for model weights, and loss history\n",
    "\n",
    "try:\n",
    "    root_path = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    root_path = os.getcwd()\n",
    "\n",
    "saved_models_path = os.path.join(root_path, 'saved_models', 'wikitext_103')\n",
    "model_name = str(model.backbone.__class__.__name__)\n",
    "date = str(datetime.datetime.today()).split()\n",
    "date_time = date[0]\n",
    "model_path = os.path.join(saved_models_path, f\"{model_name}__{ablation_name}__{date_time}\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# args.lr = 1E-2 # Learning rate\n",
    "# optimizer = torch.optim.Adam(params_list, lr=args.lr, betas=(0.9, 0.999)) # Optimizer\n",
    "\n",
    "# Separate the \"Physics\" from the \"Features\"\n",
    "sde_params = [p for n, p in model.named_parameters() if any(k in n for k in ['mu_', 'sigma_', 'eta_', 'gamma_'])]\n",
    "feature_params = [p for n, p in model.named_parameters() if not any(k in n for k in ['mu_', 'sigma_', 'eta_', 'gamma_'])]\n",
    "\n",
    "feature_lr = 1e-3\n",
    "sde_lr = feature_lr/2\n",
    "\n",
    "# optimizer = torch.optim.Adam([\n",
    "#     {'params': feature_params, 'lr': feature_lr},\n",
    "#     {'params': sde_params, 'lr': sde_lr}  # slower to prevent spikes\n",
    "# ])\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    # Standard Weights (with momentum)\n",
    "    {'params': feature_params, 'lr': feature_lr, 'betas': (0.9, 0.999)},\n",
    "    \n",
    "    # Decay Params (Lower learning rate, NO momentum, higher epsilon)\n",
    "    {\n",
    "        'params': sde_params, \n",
    "        'lr': sde_lr,          # Lower learning rate\n",
    "        'betas': (0.0, 0.999), # First beta=0 kills momentum\n",
    "        'eps': 1e-7            # Higher eps prevents division-by-zero spikes\n",
    "    }\n",
    "])\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaac5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "\n",
    "# scheduler = None\n",
    "\n",
    "################################\n",
    "\n",
    "# Cosine annealing\n",
    "\n",
    "total_steps = (args.num_epochs * len(train_dataloader)) + 10 \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, \n",
    "#     max_lr=args.lr,        # Target learning rate\n",
    "    max_lr = [feature_lr, sde_lr],\n",
    "    epochs=args.num_epochs, \n",
    "    steps_per_epoch=len(train_dataloader)+2, # Add +2 to ensure the scheduler never \"runs out\" of steps\n",
    "    pct_start=0.05,          # Spend 5% of training time warming up\n",
    "    anneal_strategy='cos',   # Use cosine decay\n",
    "    div_factor=25.0,         # Start LR is args.lr / 25\n",
    "    final_div_factor=1000.0  # Final LR is args.lr / 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f5fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If starting a new training run :\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# Initialize history tracking\n",
    "if model_name == 'TransformerNetwork':\n",
    "    history = {\n",
    "    'loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_ppl': []\n",
    "}\n",
    "else:\n",
    "    history = {\n",
    "        'loss': [], 'mu': [], 'sigma': [], 'sigma_tilde': [],\n",
    "        'eta': [], 'gamma': [], 'tau': [], 'nu_over_d': [],\n",
    "        'val_loss': [], 'val_ppl': [], 'epoch_times': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c184b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## LOAD MODEL ##\n",
    "# ################\n",
    "\n",
    "# # checkpoint_path = os.path.join(saved_models_path, 'TransformerNetwork__M0__2026-01-11')\n",
    "# checkpoint_path = os.path.join(saved_models_path, 'RFATransformerNetwork__M2.7__2026-01-27')\n",
    "\n",
    "# # Path to checkpoint\n",
    "# checkpoint_path = os.path.join(checkpoint_path, \"rfa_lm_epoch_2.pt\")\n",
    "# # checkpoint_path = os.path.join(checkpoint_path, \"standard_transformer_epoch_15.pt\")\n",
    "\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#     print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "    \n",
    "#     # Use weights_only=False because we are loading a custom dict with history/args\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=args.device)\n",
    "\n",
    "#     # Restore Model Weights\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#     # Restore Optimizer and Scheduler\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "#         scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#     # Extract History and Test Results\n",
    "#     history = checkpoint['history']\n",
    "#     start_epoch = checkpoint['epoch'] + 1\n",
    "    \n",
    "#     # Print Stored Results\n",
    "#     print(\"-\" * 30)\n",
    "#     print(f\"Resuming from Epoch: {start_epoch}\")\n",
    "#     print(f\"Last Val PPL: {history['val_ppl'][-1]:.2f}\")\n",
    "#     print('Validation history:', history['val_ppl'])\n",
    "    \n",
    "# else:\n",
    "#     print(\"No checkpoint found. Starting from scratch.\")\n",
    "#     start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHECKS\n",
    "\n",
    "# # Access the omega (frequency) tensor\n",
    "# rfa_layer = model.backbone.blocks[0].attn\n",
    "# omega = rfa_layer.omega_v \n",
    "\n",
    "# # Compare the first and last head\n",
    "# head_0 = omega[0]\n",
    "# head_last = omega[-1]\n",
    "\n",
    "# # Compute the similarity\n",
    "# are_identical = torch.equal(head_0, head_last)\n",
    "\n",
    "# print(f\"Frequencies across heads are identical: {are_identical}\")\n",
    "\n",
    "# if are_identical:\n",
    "#     print(\"CRITICAL: You are using Standard RoPE (Wrong for M8).\")\n",
    "# else:\n",
    "#     print(\"SUCCESS: Your Spectral Coupling is preserved (Right for M8).\")\n",
    "#     print(f\"Head 0 Max Freq: {head_0.max().item():.6f}\")\n",
    "#     print(f\"Head Last Max Freq: {head_last.max().item():.6f}\")\n",
    "    \n",
    "# def check_transformer_damping(model):\n",
    "#     \"\"\"\n",
    "#     Scans a Transformer model to find attention layers and \n",
    "#     calculate the effective damping ratio 'b'.\n",
    "#     \"\"\"\n",
    "#     print(f\"{'Block':<8} | {'Type':<15} | {'Avg b':<10} | {'Zero Frac':<10}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "#     # Iterate through all modules to find the attention layers\n",
    "#     for name, module in model.named_modules():\n",
    "#         # Check for Isotropic RFA (M8)\n",
    "#         if \"MultiheadIsotropicRFA\" in str(type(module)):\n",
    "#             mu = module.mu_v.detach().cpu()\n",
    "#             omega = module.omega_v.detach().cpu()\n",
    "            \n",
    "#             # Max omega for each head's shard is at the end: omega[:, -1]\n",
    "#             max_omegas = omega[:, -1]\n",
    "#             mask = mu > 0\n",
    "            \n",
    "#             avg_b = (mu[mask] / max_omegas[mask]).mean().item() if mask.any() else 0.0\n",
    "#             zero_frac = (mu == 0).float().mean().item()\n",
    "            \n",
    "#             print(f\"{name:<8} | RFA (M8)       | {avg_b:.4f}     | {zero_frac:.2f}\")\n",
    "\n",
    "#         # Check for Heuristic Attention (M14)\n",
    "#         elif \"SpectralCoupledHeuristicAttention\" in str(type(module)):\n",
    "#             mu = module.mu.detach().cpu()\n",
    "#             # Frequencies are stored in the SCRoPE or RoPE sub-module\n",
    "#             omega = module.rope.theta.detach().cpu()\n",
    "            \n",
    "#             # Handle standard RoPE (1D theta) vs SCRoPE (2D theta)\n",
    "#             if omega.ndim == 1:\n",
    "#                 max_omega = omega[-1]\n",
    "#                 mask = mu > 0\n",
    "#                 avg_b = (mu[mask] / max_omega).mean().item() if mask.any() else 0.0\n",
    "#             else:\n",
    "#                 max_omegas = omega[:, -1]\n",
    "#                 mask = mu > 0\n",
    "#                 avg_b = (mu[mask] / max_omegas[mask]).mean().item() if mask.any() else 0.0\n",
    "                \n",
    "#             zero_frac = (mu == 0).float().mean().item()\n",
    "#             print(f\"{name:<8} | Heuristic (M14)| {avg_b:.4f}     | {zero_frac:.2f}\")\n",
    "\n",
    "# # Usage:\n",
    "# check_transformer_damping(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1919fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_training_progress_lm(history, 15, checkpoint_path, save=False)\n",
    "\n",
    "# visualize_rfa_lm(model, eval_dataloader.dataset, history, 7, checkpoint_path, args,\n",
    "#                     save=False,\n",
    "#                     plot_log_losses_flag=True,\n",
    "#                     plot_last_attn_mat_flag=True,\n",
    "#                     plot_decay_per_iteration=True,\n",
    "#                     plot_noise_params = True,\n",
    "#                     plot_tau_and_nu_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6711235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "########################### RFA TRAINING LOOP ###########################\n",
    "#########################################################################\n",
    "\n",
    "if model_name == 'TransformerNetwork':\n",
    "    print('ERROR: Wrong model type.')\n",
    "\n",
    "print(f\"Starting training on {args.device}...\")\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, args.num_epochs)):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- TRAINING PHASE ---\n",
    "    model.train()\n",
    "    output_dict, history = single_epoch_rfa_lm(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        history, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        args, \n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_duration = end_time - start_time\n",
    "    history['epoch_times'].append(epoch_duration)\n",
    "    \n",
    "    # --- EVALUATION PHASE ---\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "            inputs, labels = batch[\"input_ids\"], batch[\"labels\"]\n",
    "            \n",
    "            logits, _ = model(inputs, t_measure=None, causal=True)\n",
    "            \n",
    "            # Shift logic for next-token prediction\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            \n",
    "            v_loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            val_losses.append(v_loss.item())\n",
    "\n",
    "    # --- LOGGING & METRICS ---\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_ppl = np.exp(avg_val_loss)\n",
    "    \n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "\n",
    "    # Calculate average train loss for current epoch\n",
    "    train_loss_epoch = np.mean(history['loss'][-len(train_dataloader):])\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{args.num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss_epoch:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"Val PPL: {val_ppl:.2f}\")\n",
    "\n",
    "    # --- SLIDING WINDOW CHECKPOINT SAVING ---\n",
    "    if args.save_model and (epoch + 1) % args.save_epochs == 0:\n",
    "        checkpoint_name = f\"rfa_lm_epoch_{epoch+1}.pt\"\n",
    "        checkpoint_full_path = os.path.join(model_path, checkpoint_name)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'history': history,\n",
    "            'args': args\n",
    "        }, checkpoint_full_path)\n",
    "        print(f\"Full checkpoint saved: {checkpoint_name}\")\n",
    "\n",
    "        # Keep only the 2 most recent checkpoints to save disk space\n",
    "        checkpoints = sorted(glob.glob(os.path.join(model_path, \"rfa_lm_epoch_*.pt\")), \n",
    "                             key=os.path.getmtime)\n",
    "        if len(checkpoints) > 2:\n",
    "            for i in range(len(checkpoints) - 2):\n",
    "                os.remove(checkpoints[i])\n",
    "                print(f\"Removed old checkpoint: {os.path.basename(checkpoints[i])}\")\n",
    "\n",
    "    # --- VISUALIZATION ---\n",
    "    if np.mod(epoch + 1, args.show_example_epochs) == 0:\n",
    "        plot_training_progress_lm(history, epoch, model_path)\n",
    "        \n",
    "        # Uncomment if you want the deep RFA dynamics visualizations\n",
    "        # visualize_rfa_lm(model, eval_dataloader.dataset, history, epoch, model_path, args,\n",
    "        #                  save=True,\n",
    "        #                  plot_log_losses_flag=True,\n",
    "        #                  plot_last_attn_mat_flag=True,\n",
    "        #                  plot_decay_per_iteration=True,\n",
    "        #                  plot_noise_params=True,\n",
    "        #                  plot_tau_and_nu_flag=True)\n",
    "\n",
    "# --- FINAL EVALUATION ON TEST SET ---\n",
    "print(\"\\nTraining complete. Performing final evaluation on Test Set...\")\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "criterion_sum = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "        logits, _ = model(batch[\"input_ids\"], t_measure=None, causal=True)\n",
    "        \n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = batch[\"labels\"][:, 1:].contiguous()\n",
    "        \n",
    "        # Identify non-padding tokens (ignore_index is usually -100)\n",
    "        mask = (shift_labels != -100) \n",
    "        num_tokens = mask.sum().item()\n",
    "        \n",
    "        loss_sum = criterion_sum(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), \n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        total_loss += loss_sum.item()\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "# Perplexity calculation: exp(Total NLL / Total Tokens)\n",
    "avg_nll = total_loss / total_tokens\n",
    "test_ppl = np.exp(avg_nll)\n",
    "\n",
    "history['test_loss'] = avg_nll\n",
    "history['test_ppl'] = test_ppl\n",
    "\n",
    "print(f\"Final Test Results | Avg NLL: {avg_nll:.4f} | PPL: {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jump\n",
    "\n",
    "# #########################################\n",
    "# ##### LOOP FOR STANDARD ATTENTION #######\n",
    "\n",
    "# if model_name == 'RFATransformerNetwork':\n",
    "#     print('ERROR: Wrong model type.')\n",
    "\n",
    "# print(f\"Starting Baseline Training on {args.device}...\")\n",
    "\n",
    "# for epoch in tqdm(range(start_epoch, args.num_epochs)):\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- TRAINING PHASE ---\n",
    "#     model.train()\n",
    "#     # Call the simplified standard training function\n",
    "#     attn_weights, history = single_epoch_standard_lm(\n",
    "#         model, \n",
    "#         train_dataloader, \n",
    "#         history, \n",
    "#         optimizer, \n",
    "#         criterion, \n",
    "#         args, \n",
    "#         scheduler=scheduler\n",
    "#     )\n",
    "    \n",
    "#     # --- EVALUATION PHASE ---\n",
    "#     model.eval()\n",
    "#     val_losses = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in eval_dataloader:\n",
    "#             batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "#             inputs, labels = batch[\"input_ids\"], batch[\"labels\"]\n",
    "            \n",
    "#             # Standard forward pass\n",
    "#             logits, _ = model(inputs)\n",
    "            \n",
    "#             # Internal shift for validation loss\n",
    "#             shift_logits = logits[:, :-1, :].contiguous()\n",
    "#             shift_labels = labels[:, 1:].contiguous()\n",
    "            \n",
    "#             v_loss = criterion(\n",
    "#                 shift_logits.view(-1, shift_logits.size(-1)), \n",
    "#                 shift_labels.view(-1)\n",
    "#             )\n",
    "#             val_losses.append(v_loss.item())\n",
    "\n",
    "#     # --- LOGGING & METRICS ---\n",
    "#     avg_val_loss = np.mean(val_losses)\n",
    "#     val_ppl = np.exp(avg_val_loss)\n",
    "    \n",
    "#     history['val_loss'].append(avg_val_loss)\n",
    "#     history['val_ppl'].append(val_ppl)\n",
    "\n",
    "#     # Calculate average train loss for current epoch\n",
    "#     train_loss_epoch = np.mean(history['loss'][-len(train_dataloader):])\n",
    "\n",
    "#     # Print summary for the epoch\n",
    "#     print(f\"Epoch {epoch+1}/{args.num_epochs} | \"\n",
    "#           f\"Train Loss: {train_loss_epoch:.4f} | \"\n",
    "#           f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "#           f\"Val PPL: {val_ppl:.2f}\")\n",
    "\n",
    "#     # --- SLIDING WINDOW CHECKPOINT SAVING ---\n",
    "#     if args.save_model and (epoch + 1) % args.save_epochs == 0:\n",
    "#         checkpoint_name = f\"standard_transformer_epoch_{epoch+1}.pt\"\n",
    "#         checkpoint_full_path = os.path.join(model_path, checkpoint_name)\n",
    "        \n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "#             'history': history,\n",
    "#             'args': args\n",
    "#         }, checkpoint_full_path)\n",
    "#         print(f\"Full checkpoint saved: {checkpoint_name}\")\n",
    "\n",
    "#         # Keep only the 2 most recent baseline checkpoints to save disk space\n",
    "#         checkpoints = sorted(glob.glob(os.path.join(model_path, \"standard_transformer_epoch_*.pt\")), \n",
    "#                              key=os.path.getmtime)\n",
    "#         if len(checkpoints) > 2:\n",
    "#             for i in range(len(checkpoints) - 2):\n",
    "#                 os.remove(checkpoints[i])\n",
    "#                 print(f\"Removed old baseline checkpoint: {os.path.basename(checkpoints[i])}\")\n",
    "\n",
    "#     # Plotting (Using the same progress plotter, but with baseline history)\n",
    "#     if np.mod(epoch + 1, args.show_example_epochs) == 0:\n",
    "#         plot_training_progress_lm(history, epoch, model_path)\n",
    "        \n",
    "# # --- AFTER THE FULL LOOP FINISHES ---\n",
    "# print(\"\\nBaseline training complete. Performing final evaluation on Test Set...\")\n",
    "# model.eval()\n",
    "\n",
    "# # Use 'sum' reduction to get total negative log-likelihood\n",
    "# total_loss = 0.0\n",
    "# total_tokens = 0\n",
    "# criterion_sum = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_dataloader:\n",
    "#         batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "#         # Standard forward pass\n",
    "#         logits, _ = model(batch[\"input_ids\"])\n",
    "        \n",
    "#         # Shift logic\n",
    "#         shift_logits = logits[:, :-1, :].contiguous()\n",
    "#         shift_labels = batch[\"labels\"][:, 1:].contiguous()\n",
    "        \n",
    "#         # Identify non-padding tokens\n",
    "#         mask = (shift_labels != -100) \n",
    "#         num_tokens = mask.sum().item()\n",
    "        \n",
    "#         # Calculate sum of losses for this batch\n",
    "#         loss_sum = criterion_sum(\n",
    "#             shift_logits.view(-1, shift_logits.size(-1)), \n",
    "#             shift_labels.view(-1)\n",
    "#         )\n",
    "        \n",
    "#         total_loss += loss_sum.item()\n",
    "#         total_tokens += num_tokens\n",
    "\n",
    "# # PPL: exp( (sum of all losses) / (total tokens) )\n",
    "# avg_nll = total_loss / total_tokens\n",
    "# test_ppl = np.exp(avg_nll)\n",
    "\n",
    "# history['test_loss'] = avg_nll \n",
    "# history['test_ppl'] = test_ppl\n",
    "\n",
    "# print(f\"Final Baseline Test Results | Avg NLL: {avg_nll:.4f} | PPL: {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bddb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AFTER THE FULL LOOP FINISHES ---\n",
    "print(\"Training complete. Performing final evaluation on Test Set...\")\n",
    "model.eval()\n",
    "\n",
    "# Use 'sum' reduction to get total negative log-likelihood\n",
    "total_loss = 0.0\n",
    "total_tokens = 0\n",
    "criterion_sum = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(args.device) for k, v in batch.items()}\n",
    "        logits, _ = model(batch[\"input_ids\"], t_measure=None, causal=True)\n",
    "        \n",
    "        # Shift logic\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = batch[\"labels\"][:, 1:].contiguous()\n",
    "        \n",
    "        # Identify non-padding tokens (ignore_index assumes -100 or pad_id)\n",
    "        # If labels use a specific pad_id, replace -100 with that id.\n",
    "        mask = (shift_labels != -100) \n",
    "        num_tokens = mask.sum().item()\n",
    "        \n",
    "        # Calculate sum of losses for this batch\n",
    "        loss_sum = criterion_sum(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), \n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        total_loss += loss_sum.item()\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "# PPL: exp( (sum of all losses) / (total tokens) )\n",
    "avg_nll = total_loss / total_tokens\n",
    "test_ppl = np.exp(avg_nll)\n",
    "\n",
    "history['test_loss'] = avg_nll # This is the average loss per token\n",
    "history['test_ppl'] = test_ppl\n",
    "\n",
    "print(f\"Final Test Results | Avg NLL: {avg_nll:.4f} | PPL: {test_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef2e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
